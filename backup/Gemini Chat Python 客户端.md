# Gemini Chat Python å®¢æˆ·ç«¯

è¿™æ˜¯ä¸€ä¸ªåŸºäº Python å®ç°çš„ Gemini AI èŠå¤©å®¢æˆ·ç«¯ï¼Œæä¾›äº†æ›´å¤šé«˜çº§åŠŸèƒ½å’Œæ›´å¥½çš„å¼‚æ­¥æ€§èƒ½ã€‚

## åŠŸèƒ½ç‰¹ç‚¹

- ğŸ”„ è‡ªåŠ¨è·å–æœ€æ–°çš„ Gemini æ¨¡å‹åˆ—è¡¨
- ğŸ¤– æ”¯æŒå¤šä¸ª Gemini æ¨¡å‹ç³»åˆ—ï¼ˆPro/Flashï¼‰
- ğŸ’¾ æ”¯æŒå¯¹è¯å†å²è®°å½•ï¼ˆé»˜è®¤ä¿ç•™æœ€è¿‘10è½®å¯¹è¯ï¼‰
- ğŸ”„ æ”¯æŒå®æ—¶åˆ‡æ¢å’Œåˆ·æ–°æ¨¡å‹
- ğŸ§¹ æ”¯æŒæ¸…ç©ºå¯¹è¯å†å²
- ğŸ“¦ è‡ªåŠ¨æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–
- âš¡ å¼‚æ­¥å®ç°ï¼Œå“åº”æ›´å¿«
- ğŸ›¡ï¸ å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶

## ç³»ç»Ÿè¦æ±‚

- Python 3.7+
- pipï¼ˆPythonåŒ…ç®¡ç†å™¨ï¼‰
- ç½‘ç»œè¿æ¥

## ä¾èµ–é¡¹

è„šæœ¬ä¼šè‡ªåŠ¨æ£€æŸ¥å¹¶å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š
- aiohttpï¼šç”¨äºå¼‚æ­¥HTTPè¯·æ±‚
- asyncioï¼šç”¨äºå¼‚æ­¥ç¼–ç¨‹æ”¯æŒ

## å®‰è£…å’Œè¿è¡Œ

1. ä¸‹è½½è„šæœ¬ï¼š
ç›´æ¥å¤åˆ¶è„šæœ¬å†…å®¹åˆ°æœ¬åœ°æ–‡ä»¶

2. è¿è¡Œè„šæœ¬ï¼š
```bash
python3 gemini_chat.py
```

é¦–æ¬¡è¿è¡Œæ—¶ï¼Œå¦‚æœç¼ºå°‘å¿…è¦çš„ä¾èµ–é¡¹ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨å°è¯•å®‰è£…ã€‚

## ä½¿ç”¨è¯´æ˜

### åŸºæœ¬æ“ä½œ

1. å¯åŠ¨åï¼Œè„šæœ¬ä¼šè‡ªåŠ¨è·å–å¹¶æ˜¾ç¤ºå¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
2. å¯ä»¥é€šè¿‡è¾“å…¥ç¼–å·é€‰æ‹©æ¨¡å‹ï¼Œæˆ–ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼ˆæœ€æ–°ç‰ˆæœ¬ï¼‰
3. åœ¨èŠå¤©ç•Œé¢ä¸­æ”¯æŒä»¥ä¸‹å‘½ä»¤ï¼š
   - ç›´æ¥è¾“å…¥æ–‡å­—è¿›è¡Œå¯¹è¯
   - è¾“å…¥ `é€€å‡º` æˆ– `quit` æˆ– `exit` ç»“æŸå¯¹è¯
   - è¾“å…¥ `åˆ‡æ¢` å¯ä»¥åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å‹
   - è¾“å…¥ `åˆ·æ–°` å¯ä»¥åˆ·æ–°æ¨¡å‹åˆ—è¡¨
   - è¾“å…¥ `æ¸…ç©º` å¯ä»¥æ¸…ç©ºå¯¹è¯å†å²

### é«˜çº§åŠŸèƒ½

1. å¯¹è¯å†å²è®°å½•
   - è‡ªåŠ¨ä¿å­˜æœ€è¿‘10è½®å¯¹è¯
   - åˆ‡æ¢æ¨¡å‹æ—¶è‡ªåŠ¨æ¸…ç©ºå†å²
   - å¯ä»¥éšæ—¶æ‰‹åŠ¨æ¸…ç©ºå†å²

2. å¼‚æ­¥å¤„ç†
   - ä½¿ç”¨ aiohttp è¿›è¡Œå¼‚æ­¥HTTPè¯·æ±‚
   - æä¾›æ›´å¥½çš„å“åº”æ€§èƒ½

3. é”™è¯¯å¤„ç†
   - ç½‘ç»œé”™è¯¯è‡ªåŠ¨é‡è¯•
   - å‹å¥½çš„é”™è¯¯æç¤º
   - ä¼˜é›…çš„ç¨‹åºé€€å‡ºå¤„ç†

### æ¨¡å‹è¯´æ˜

- pro ç³»åˆ—ï¼šå“åº”è´¨é‡æ›´é«˜ï¼Œé€‚åˆéœ€è¦æ·±åº¦æ€è€ƒçš„åœºæ™¯
- flash ç³»åˆ—ï¼šå“åº”é€Ÿåº¦æ›´å¿«ï¼Œé€‚åˆæ—¥å¸¸å¯¹è¯
- ç‰ˆæœ¬å·è¶Šå¤§ä»£è¡¨æ¨¡å‹è¶Šæ–°

## å¼€å‘è€…è¯´æ˜

### ä»£ç ç»“æ„

- `GeminiChat` ç±»ï¼šä¸»è¦çš„èŠå¤©å®ç°
  - `check_dependencies()`: ä¾èµ–æ£€æŸ¥
  - `fetch_models()`: è·å–æ¨¡å‹åˆ—è¡¨
  - `chat_completion()`: å‘é€èŠå¤©è¯·æ±‚
  - `save_conversation()`: ä¿å­˜å¯¹è¯å†å²
  - `get_conversation_context()`: è·å–å¯¹è¯ä¸Šä¸‹æ–‡

### ç±»å‹æ³¨è§£

ä»£ç ä½¿ç”¨äº† Python ç±»å‹æ³¨è§£ï¼Œæä¾›æ›´å¥½çš„ä»£ç æç¤ºå’Œé”™è¯¯æ£€æŸ¥ï¼š
```python
from typing import Dict, List, Optional
```

## æ³¨æ„äº‹é¡¹

1. ç¡®ä¿ Python ç‰ˆæœ¬ >= 3.7
2. é¦–æ¬¡è¿è¡Œæ—¶éœ€è¦ç½‘ç»œè¿æ¥ä»¥å®‰è£…ä¾èµ–
3. API å¯†é’¥å·²å†…ç½®ï¼Œæ— éœ€é¢å¤–é…ç½®
4. å»ºè®®åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œ

## å¸¸è§é—®é¢˜

1. å¦‚æœé‡åˆ°ä¾èµ–å®‰è£…å¤±è´¥ï¼š
   ```bash
   pip install aiohttp asyncio
   ```

2. å¦‚æœé‡åˆ°æƒé™é—®é¢˜ï¼š
   ```bash
   sudo python3 gemini_chat.py
   ```

3. å¦‚æœé‡åˆ°ç½‘ç»œé—®é¢˜ï¼š
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - ç¡®è®¤ API æœåŠ¡å™¨å¯è®¿é—®
   - æ£€æŸ¥é˜²ç«å¢™è®¾ç½®

## æ›´æ–°æ—¥å¿—

### v1.0.0
- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ”¯æŒåŠ¨æ€è·å–æ¨¡å‹åˆ—è¡¨
- æ·»åŠ å¯¹è¯å†å²åŠŸèƒ½
- æ”¯æŒæ¨¡å‹åˆ‡æ¢å’Œåˆ·æ–°
- æ·»åŠ è‡ªåŠ¨ä¾èµ–å®‰è£…
- æ·»åŠ ç±»å‹æ³¨è§£
- ä¼˜åŒ–é”™è¯¯å¤„ç†

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import aiohttp
import asyncio
import json
import sys
import pkg_resources
import subprocess
from typing import Dict, List, Optional

class GeminiChat:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://gemini.tangrenbin.us.kg"
        self.messages = []
        self.conversation_history = []  # å­˜å‚¨å¯¹è¯å†å²
        self.max_history = 10  # é»˜è®¤ä¿ç•™æœ€è¿‘10è½®å¯¹è¯

    @staticmethod
    def check_dependencies() -> None:
        """æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„ä¾èµ–"""
        required = {'aiohttp', 'asyncio'}
        installed = {pkg.key for pkg in pkg_resources.working_set}
        missing = required - installed

        if missing:
            print("æ£€æµ‹åˆ°ç¼ºå°‘å¿…è¦çš„ä¾èµ–é¡¹ï¼Œæ­£åœ¨å®‰è£…...")
            try:
                subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing])
                print("ä¾èµ–å®‰è£…å®Œæˆï¼")
            except subprocess.CalledProcessError:
                print("é”™è¯¯ï¼šä¾èµ–å®‰è£…å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å®‰è£…ä»¥ä¸‹åŒ…ï¼š")
                print("\n".join(missing))
                sys.exit(1)

    async def fetch_models(self) -> Dict[str, str]:
        """ä»APIè·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        print("æ­£åœ¨è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨...")
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.base_url}/v1/models",
                    headers={'Authorization': f'Bearer {self.api_key}'}
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        models = {}
                        index = 1
                        for model in data['data']:
                            if 'gemini' in model['id']:
                                models[str(index)] = model['id']
                                index += 1
                        return models
                    else:
                        error_text = await response.text()
                        raise Exception(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {error_text}")
        except Exception as e:
            print(f"é”™è¯¯ï¼šæ— æ³•è·å–æ¨¡å‹åˆ—è¡¨ - {str(e)}")
            sys.exit(1)

    def print_models(self, models: Dict[str, str], default_model: str) -> None:
        """æ‰“å°æ¨¡å‹åˆ—è¡¨"""
        print("\nå¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼š")
        print("æ³¨æ„ï¼šä¸åŒæ¨¡å‹æœ‰ä¸åŒçš„ç‰¹ç‚¹ï¼š")
        print("- proç³»åˆ—ï¼šå“åº”è´¨é‡æ›´é«˜")
        print("- flashç³»åˆ—ï¼šå“åº”é€Ÿåº¦æ›´å¿«")
        print("- æ•°å­—è¶Šå¤§ä»£è¡¨ç‰ˆæœ¬è¶Šæ–°")
        print(f"- é»˜è®¤ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹: {models[default_model]}\n")

        for key, model in models.items():
            print(f"{key}. {model}" + (" (é»˜è®¤)" if key == default_model else ""))

    def save_conversation(self, role: str, content: str) -> None:
        """ä¿å­˜å¯¹è¯å†å²"""
        self.conversation_history.append({"role": role, "content": content})
        if len(self.conversation_history) > self.max_history * 2:  # æ¯è½®å¯¹è¯åŒ…å«ç”¨æˆ·å’ŒAIä¸¤æ¡æ¶ˆæ¯
            self.conversation_history = self.conversation_history[-self.max_history * 2:]

    def get_conversation_context(self) -> List[Dict[str, str]]:
        """è·å–å¯¹è¯ä¸Šä¸‹æ–‡"""
        return [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"}] + self.conversation_history

    async def chat_completion(self, model: str, temperature: float = 0.7) -> str:
        """å‘é€èŠå¤©è¯·æ±‚"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/v1/chat/completions",
                    headers={
                        'Content-Type': 'application/json',
                        'Authorization': f'Bearer {self.api_key}'
                    },
                    json={
                        "model": model,
                        "messages": self.get_conversation_context(),
                        "stream": False,
                        "temperature": temperature,
                        "max_tokens": 2048
                    }
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data['choices'][0]['message']['content']
                    else:
                        error_text = await response.text()
                        raise Exception(f"APIé”™è¯¯: {error_text}")
        except Exception as e:
            return f"é”™è¯¯ï¼š{str(e)}"

    async def chat(self) -> None:
        """ä¸»å¯¹è¯å¾ªç¯"""
        # è·å–æ¨¡å‹åˆ—è¡¨
        models = await self.fetch_models()
        default_model = str(len(models))  # ä½¿ç”¨æœ€åä¸€ä¸ªæ¨¡å‹ä½œä¸ºé»˜è®¤å€¼
        
        while True:
            self.print_models(models, default_model)
            print(f"\nç›´æ¥æŒ‰å›è½¦ä½¿ç”¨é»˜è®¤æ¨¡å‹({models[default_model]})")
            model_choice = input("è¯·é€‰æ‹©æ¨¡å‹ç¼–å·: ").strip()
            
            if not model_choice:
                model_choice = default_model
                
            if model_choice in models:
                break
            print("æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡è¯•ã€‚")
        
        model = models[model_choice]
        print(f"\nå·²é€‰æ‹©æ¨¡å‹: {model}")
        print("\nå¼€å§‹å¯¹è¯ (è¾“å…¥'é€€å‡º'ç»“æŸå¯¹è¯ï¼Œè¾“å…¥'åˆ‡æ¢'å¯ä»¥åˆ‡æ¢æ¨¡å‹ï¼Œè¾“å…¥'åˆ·æ–°'å¯ä»¥åˆ·æ–°æ¨¡å‹åˆ—è¡¨ï¼Œè¾“å…¥'æ¸…ç©º'å¯ä»¥æ¸…ç©ºå¯¹è¯å†å²)")
        
        while True:
            user_input = input("\nä½ : ").strip()
            
            if user_input.lower() in ['é€€å‡º', 'quit', 'exit']:
                print("\næ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼")
                break
                
            if user_input.lower() == 'åˆ‡æ¢':
                self.conversation_history = []  # åˆ‡æ¢æ¨¡å‹æ—¶æ¸…ç©ºå¯¹è¯å†å²
                await self.chat()
                return
                
            if user_input.lower() == 'åˆ·æ–°':
                models = await self.fetch_models()
                self.print_models(models, model_choice)
                print(f"\nç»§ç»­ä½¿ç”¨å½“å‰æ¨¡å‹: {model}")
                continue
                
            if user_input.lower() == 'æ¸…ç©º':
                self.conversation_history = []
                print("\nå¯¹è¯å†å²å·²æ¸…ç©º")
                continue
            
            if user_input:
                # ä¿å­˜ç”¨æˆ·è¾“å…¥
                self.save_conversation("user", user_input)
                
                print("\næ­£åœ¨ç­‰å¾…AIå“åº”...")
                response = await self.chat_completion(model)
                print(f"\nAI: {response}")
                
                # ä¿å­˜AIå›å¤
                self.save_conversation("assistant", response)

async def main():
    # æ£€æŸ¥ä¾èµ–
    GeminiChat.check_dependencies()
    
    api_key = "AIzaSyDOLKXvMTqw0EgLHVOA13fgBZHbS8NEvBs"
    chat = GeminiChat(api_key)
    await chat.chat()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nç¨‹åºå·²è¢«ç”¨æˆ·ä¸­æ–­ã€‚å†è§ï¼")
    except Exception as e:
        print(f"\nç¨‹åºé‡åˆ°é”™è¯¯: {str(e)}") 
```
